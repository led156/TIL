<img width="493" alt="image" src="https://github.com/led156/TIL/assets/67251510/925bdc8e-95ff-458a-a40d-70377f6f9b65">


# 4.1. 벌크 형과 스트리밍 형의 데이터 수집
## 객체 스토리지와 데이터 수집
- 빅데이터는 대부분 확장성이 높은 '분산 스토리지(distributed storage)'에 저장됨.
  <img width="593" alt="image" src="https://github.com/led156/TIL/assets/67251510/d7c9feb9-acb9-4b4a-9c62-1202cfa8d295">

  + 분산 형의 데이터베이스가 이용되는 경우도 있지만, 기본은 대량으로 파일을 저장하기 위해 '객체 스토리지(object storage)'임.
  + Hadoop의 'HDFS', 클라우드 서비스로는 'Amazon S3'
  + 객체 스토리지에서의 파일 읽고 쓰기는 네트워크를 거쳐서 실행함.
    * 내부 처리에 다수의 물리적인 서버와 하드 디스크가 있음.
    * 데이터는 여러 디스크에 복사되기 때문에 일부 하드웨어가 고장나더라도 데이터가 손실되지 않는다.
    * 데이터의 읽고 쓰기를 다수 하드웨어에 분산함으로써 데이터의 양이 늘어나도 성능이 떨어지는 일이 없도록 고안되어 있다.
  + 객체 스토리지의 구조는 데이터양이 많을 때 우수하지만, 소량의 데이터에는 비효율적임에 주의.
    * 예시) 100바이트의 작은 파일을 자주 읽고 쓰는 것은 적합X. 데이터양에 비해 통신 오버헤드가 크기 때문.

### 데이터 수집
- 데이터 수집(data ingesion) : 수집한 데이터를 가공하여 집계 효율이 좋은 분산 스토리지를 만드는 일련의 프로세스. (데이터 수집부터~ 구조화 데이터의 작성, 분산 스토리지에 대한 장기적 저장)
- 빅데이터로 자주 다루는 것은 시간과 함께 생성되는 데이터임 (시계열 데이터)
- 이를 수시로 객체 스토리지에 기록하면 대량의 작은 파일이 생성됨 → 시간이 지남에 따라 성능 저하의 요인이 됨. ⇒ 작은 데이터를 적당히 모아 하나의 큰 파일로 만들어 효율을 높인다.
- 반대로, 지나치게 파일이 커도 문제가 있음.
  + 파일 크기가 증가하면 네트워크 전송에 시간이 걸려 예상치 못한 오류 발생률이 높아짐.
- 단순히 수집보다 나중에 처리하기 쉽도록 준비해 둘 필요가 있음.
  + 객체 스토리지에서 효율적으로 처리할 수 있는 파일 크기는 1MB~1GB 사이이다.

## 벌크 형의 데이터 전송
- 전통적인 데이터 웨어하우스에서 사용.
  + 데이터베이스나 파일 서버/웹 서비스 등에서 각각의 방식(SQL, API 등)으로 정리해 데이터를 추출.
  + 과거 축적된 대량의 데이터 / 기존 데이터를 추출하고 싶을 경우에도 사용.
- 데이터가 처음부터 분산 스토리지에 저장되어 있지 않다면, 데이터 전송을 위한 'ETL 서버'를 설치함.
  <img width="482" alt="image" src="https://github.com/led156/TIL/assets/67251510/427f5d11-d000-4283-bdb6-d10fd573f64f">

  + ELT 서버 : 구조화된 데이터 처리에 적합하고, 데이터 웨어하우스를 위한 ETL 도구 & 오픈 소스의 벌크 전송 도구 또는 손수 작성한 스크립트 등을 사용 가능.

### 파일 사이즈의 적정화는 비교적 간단하다
- ETL 프로세스는 1시간~하루마다 간격으로 정기적 실행을 함 → 파일 사이즈를 자동으로 적정화.
- 그렇게 사용하고 있지 않다면 전송 방법을 검토
  + 예시) 100개의 파일을 전송할 때 100번 전송하는 것이 아닌, 모아서 전송. = 한 번의 전송에 모든 파일을 포함
  + 데이터 양이 많을 때는 단위를 조절해 작은 태스크로 분해. (워크플로 관리 도구 사용)
 
### 데이터 전송의 워크플로
- 데이터 전송의 신뢰성이 중요한 경우에는 벌크형 도구를 주로 사용.
  + 스트리밍형의 경우 데이터 전송 재실행이 쉽지 않음.
  + 벌크형 전송의 장점 → 문제가 발생했을 때 전송을 재실행할 수 있음.
  + ⇒ 과거의 데이터를 빠짐없이 가져오거나, 실패한 작업을 재실행할 것을 고려하면 .. 벌크형 전송으로!
- 벌크 형 데이터 전송은 워크플로 관리 도구와의 궁합이 뛰어남.
  + 정기적인 스케줄 실행/오류 통지등을 워크플로 관리 도구에 맡김.
  + 매일 매일 마스터 데이터 스냅샷 & 신뢰성이 중시되는 과금 데이터 전송 등은 다른 배치 처리와 함께 워크플로의 일부에 포함시키는 것이 좋음 ❓

## 스트리밍 형의 데이터 전송
<img width="607" alt="image" src="https://github.com/led156/TIL/assets/67251510/fbe60b40-af96-4dfd-b59d-c893bc5b1f08">

- 대다수 데이터는 통신 장비 및 소프트웨어에 의해 생성, 그리고 네트워크를 거쳐 전송됨. (웹 브라우저, 모바일 앱, 각종 디바이스 등)
  + 지금 바로 생성되어 어디에도 저장되지 않은 데이터를 바로 전송 : 스트리밍 형 데이터 전송이 필요.
- 이러한 데이터 전송의 공통점
  + 메시지 배송(message delivery) : 다수의 클라이언트에서, 계속해서 작은 데이터가 전송됨.
  + 전송되는 데이터양에 비해 통신을 위한 오버헤드가 큼. → 처리하는 서버의 성능이 높아야 함.
- 보내온 메시지를 저장하는 방법
  1. 작은 데이터 쓰기에 적합한 NoSQL 데이터베이스 : Hive와 같은 쿼리 엔진으로 NoSQL 데이터베이스에 연결해 데이터를 읽는다.
  2. 메시지 큐(message queue), 메시지 브로커(message broker) 등의 중계 시스템에 전송 : 등록된 데이터를 일정한 간격으로 꺼내고 모아서 함께 분산 스토리지에 저장한다.
 
### 웹 브라우저에서의 메시지 배송 : Fluentd, Logstash, 웹 이벤트 트래킹
<img width="400" alt="image" src="https://github.com/led156/TIL/assets/67251510/5ac4b66a-5035-4e7e-b860-99a95451966f">

- ❶ 웹 서버 안에서 메시지를 만들어 배송
  + 자체 개발한 웹 애플리케이션 등에서 주로 사용
  + 전송 효율을 높이기 위해 서버상에서 데이터를 축적해 놓고 나중에 모아서 보내는 경우가 많음.
  + Fluentd, Logstash 같은 서버 상주형 로그 수집 소프트웨어가 주로 사용됨.
    * Fluentd에 의한 메시지 배송
    * 
      <img width="388" alt="image" src="https://github.com/led156/TIL/assets/67251510/11930ab6-1a33-4cff-93e4-7c6f8ee990e5">

      - 분산 스토리지에 데이터를 중계하는 메시지 브로커의 역할로 사용.
        + 원래 메시지 브로커로 설계된 것이 아니기 때문에, 한계 존재
          1. 여러 대로 데이터를 복제할 수 없음 : 노드가 고장 나서 버퍼가 사라진다면 보내지 못한 데이터가 없어짐 (디스크 상 버퍼가 사라지지 않는 한 재전송할 수는 있다.)
          2. 메시지를 일방적으로 발송하는 것밖에 못함 : 외부에서 요청해서 메시지를 꺼낼 수 없음.
          3. 배송에 성공한 메시지는 곧 사라져 버리기 때문에 나중에 다시 송신할 수 없음.
      - 내부에 효율적인 버퍼링 메커니즘을 갖고 있음 : 일정 시간 간격/특정 사이즈에 외부로 데이터를 모아 내보낼 수 있음.
      - 필요에 따라 부분적으로 데이터를 바꾸어 쓰거나 복수의 스토리지에 복사할 수 있음.
        
- ❷ 자바스크립트를 사용하여 웹 브라우저에서 직접 메시지를 보냄 : 웹 이벤트 추적(web event tracking)
  + 사용자 측면에서 HTML 페이지에 태그를 삽입만 하면 되어 각종 액세스 분석 서비스 / 데이터 분석 서비스 등에서 사용됨.
  + 수집된 데이터는 그대로 다른 서버로 전송되거나 API 경유로 함께 취득해, 분산 스토리지에 저장함.

### 모바일 앱으로부터의 메시지 배송 : MBaas, SDK


# 4.2.


# 4.3.


# 4.4.


