# 1.1. [배경] 빅데이터의 정착
## 분산 시스템에 의한 데이터 처리의 고속화
- 빅데이터의 취급이 어려운 이유
  1. 데이터의 분석 방법을 모른다
  2. 데이터 처리에 수고와 시간이 걸린다
  - 데이터가 있어도 그 가치를 창조하지 못한다면 의미가 없고, 지식이 있어도 시간을 많이 소비한다면 할 수 있는 것은 한정됨.

### 빅데이터 기술의 요구
- 웹 서버 등에서 생성된 데이터는 처음에는 RDB, NoSQL 등의 텍스트 데이터에 저장된다.
- 그 후 모든 데이터가 Hadoop으로 모이고, 거기서 대규모 데이터 처리가 실행된다.
### Hadoop
- 다수의 컴퓨터에서 대량의 데이터를 처리하기 위한 시스템.
  + 원래 구글 분산 처리 프레임워크 'MapReduce'를 참고해 제작됨. 초기 Hadoop에서 MapReduce를 동작시키기 위해 자바로 프로그래밍 해야 했음.
  + SQL 같은 쿼리 언어를 Hadoop에서 실행하기 위한 소프트웨어로 'Hive'가 개발됨.
### NoSQL 데이터베이스
- 전통적인 RDB의 제약을 제거하는 것을 목표로 한 데이터베이스. 빈번한 읽기/쓰기 및 분산 처리가 강점.
  + KVS(key-value store) : 다수의 키와 값을 관련지어 저장
  + document store : JSON 같은 복잡한 데이터 구조를 저장
  + wide-column store : 여러 키를 사용하여 높은 확장성을 제공
- 💡 RDB vs. NoSQL ([🔗](https://hyuuny.tistory.com/158))
  | |RDB|NoSQL|
  |--|--|--|
  |데이터 저장 모델|table|key-value, document, wide-column 등 (비정형)|
  |개발 목적|데이터 중복 감소|애자일/확장가능성/수정가능성|
  |예시|Oracle, MySQL, PostgreSQL 등|Mongo DB, Redis 등|
  |Schema|엄격한 데이터 구조|유연한 데이터 구조|
  |장점|명확한 데이터 구조 보장</br>데이터 무결성(중복없이)</br>데이터 중복이 없어 데이터 Update 용이|유연하고 자유로운 데이터 구조</br>새로운 필드 추가 자유로움</br>수평적 확장(Scale out) 용이|
  |단점|시스템이 커지면 Join문이 많은 복잡한 쿼리가 됨</br>수직적 확장(Scale up)이 주로 사용됨|데이터 중복 발생 가능</br>중복 데이터로 인해 데이터 변경 시 모든 컬렉션에서 수정 필요</br>명확한 데이터 구조를 보장하지 않음.|
  |사용|데이터 구조가 명확한 경우</br>데이터 update가 잦은 시스템|테이터 구조가 명확하지 않은 경우</br>데이터 update가 잦지 않은 경우</br>데이터 양이 매우 많은 경우|

  + 수직적 확장 vs. 수평적 확장 ([🔗](https://hyuuny.tistory.com/158](https://hudi.blog/scale-up-vs-scale-out/)))
    ![image](https://github.com/led156/TIL/assets/67251510/6e5683bd-f238-4469-a86a-2ad50d6fcf0f)

- RDB는 엄격한 schema로 인해 데이터 중복이 없기 때문에 데이터 update가 많을 때 유리하고, NoSQL은 데이터 중복으로 인해 update 시 모든 컬렉션에서 수정이 필요하기 때문에 update가 적고 조회가 많을 떄 유리하다.
- 모여진 데이터를 나중에 집계하는 것이 목적인 Hadoop과 다르게 NoSQL은 애플리케이션에서 온라인으로 접속하는 데이터베이스임.

## 분산 시스템의 비즈니스 이용 개척
① 예전의 데이터 웨어하우스 : 업무 시스템 → RDB → 데이터웨어하우스
② 데이터 웨어하우스의 증가 : 업무 시스템 → RDB → Hadoop → 데이터웨어하우스
- 데이터웨어하우스(Data Warehouse) : 데이터를 분석하고 보고하는 데 사용되는 엔터프라이즈 시스템
  + 안정적인 성능을 실현하기 위해 HW+SW 통합 장비로 제공. (확장이 어려움)
  + 따라서 대량의 데이터 처리는 Hadoop에 맡기고, 비교적 작은 데이터, 또는 중요한 데이터만을 데이터 웨어하우스에 넣는 식으로 구분하게 됨.
 
## 직접 할 수 있는 데이터 분석 폭 확대
- 여러 컴퓨터에 분산 처리한다는 점이 빅데이터의 특징. 하지만 이를 위한 하드웨어를 마련하는 일은 간단하지 않음.
- 따라서 클라우드 서비스의 보급에 의해 빅데이터 활용이 증가함.
  |시기|이벤트|서비스의 특징|
  |--|--|--|
  |2009년|Amazon Elastic MapReduce 발표|클라우드를 위한 Hadoop|
  |2010년|구글 BigQuery 발표|데이터 웨어하우스|
  |2012년|Azure HDInsight 발표|클라우드를 위한 Hadoop|
  |2012년|Amazon Redshift 발표|데이터 웨어하우스|

### 데이터 디스커버리의 기초지식
- 데이터 디스커버리(data discovery) : 데이터 웨어하우스에 저장된 데이터를 시각화하려는 방법. 대화형으로 데이터를 시각화하여 가치 있는 정보를 찾으려고 하는 프로세스. (=셀프서비스용 BI 도구)
  + BI 도구: 예전부터 데이터 웨어하우스와 조합되어 사용된 경영자용 시각화 시스템.

# 1.2. 빅데이터 시대의 데이터 분석 기반
- 빅데이터 기술이 기존 데이터 데이터 웨어하우스와 다른 점 : 다수의 분산 시스템을 조합하여 확장성이 뛰어난 데이터 처리 구조를 만든다.

## [재입문] 빅데이터의 기술
### 데이터 파이프라인
- 데이터 파이프라인(data pipeline) : 차례대로 전달해나가는 데이터로 구성된 시스템.
- <img width="472" alt="image" src="https://github.com/led156/TIL/assets/67251510/68b7725d-5f86-4ea5-99e4-b335be6f30db">
### (1) 데이터 수집
- 데이터는 여러 장소에서 발생하고 다른 형태를 보임. → 각각 서로 다른 기술로 데이터를 전송함.
- 데이터 전송 방법
  - ❶ 벌크(bulk)형
      + 이미 어딘가에 존재하는 데이터를 정리해 추출
      + 데이터베이스와 파일 서버 등에서 정기적으로 데이터를 수집하는 데에 사용함.
  - ❷ 스트리밍(streaming)형
      + 차례로 생성되는 데이터를 끊임없이 계속 보내는 방법.
      + 모바일 애플리케이션, 임베디드 장비 등에서 데이터를 수집하는 데에 사용함.
        
### (2) 스트림 처리와 배치 처리
- 스트림 처리(stream processing) : 스트리밍형으로 받은 데이터를 실시간으로 처리하는 것.
  + ❸ 시계열 데이터베이스 : 실시간 처리를 지향한 데이터베이스. 스트림 처리의 결과를 저장함으로써, 무슨 일이 일어나는지 즉시 알 수 있다.
- ❹, ❺ 배치 처리(batch processing) : 대량의 데이터를 저장하고 처리하는 데 적합한 분산 시스템.
  + 스트림 처리는 장기적인 데이터 분석에 적합하지 않음.(데이터양이 쉽게 증가하므로) → 배치 처리 구조 필요
 
### (3) 분산 스토리지 : 객체 스토리지, NoSQL 데이터베이스
- ❷, ❹ 분산 스토리지(distribute storage) : 여러 컴퓨터와 디스크로부터 구성된 스토리지 시스템.
- 데이터 저장 방법
  - 객체 스토리지(Object storage) : 한 덩어리로 모인 데이터에 이름을 부여해서 파일로 저장. (e.g. Amazon S3)
  - NoSQL 데이터베이스 : 많은 데이터를 읽고 쓸 때 성능면에서 우수. 단, 데이터 용량을 얼마든지 늘릴 수 있는 확장성이 높은 제품을 사용해야함.

### (4) 분산 데이터 처리 : 쿼리 엔진, ETL 프로세스
- 분산 스토리지에 저장된 데이터를 처리하는 데는 ❻, ❼ '분산 데이터 처리(distribute data processing)'의 프레임워크가 필요.
  + MapReduce가 사용되어진 부분. 데이터양과 처리 내용에 따라 많은 컴퓨터 자원이 필요하게 됨.
  + 분산 데이터 처리의 주 역할 : 나중에 분석하기 쉽도록 데이터를 가공해서 결과를 일부 데이터베이스에 저장하는 것.
- 데이터 집계에 있어서 SQL을 사용하는 방법
  + 1. 쿼리 엔진(query engine) : 분산 스토리지 상의 데이터를 SQL로 집계하기 위해 도입. (Hive와 같은)
      * Hive보다 고속인 '대화형 쿼리 엔진(interactive query engine)'도 개발됨 (Presto)
  + 2. ETL(extract-transform-load) 프로세스 : 데이터 웨어하우스 제품을 이용하기 위해, 분산 스토리지에서 추출한 데이터를 데이터 웨어하우스에 적합한 형식으로 변환하는 것.
      * 데이터를 추출하고, 그것을 가공한 후, 데이터 웨어하우스에 로드한다.
      * <img width="736" alt="image" src="https://github.com/led156/TIL/assets/67251510/32e7c85b-1126-466f-896e-2c5cd994b0a2">


### (5) 워크플로 관리
- 워크플로 관리(workflow management) : 전체 데이터 파이프라인의 동작을 관리. 매일 정해진 시간에 배치 처리를 스케줄대로 실행하고, 오류가 발생한 경우에는 관리자에게 통지하는 목적으로 사용.


## 데이터 웨어하우스와 데이터 마트 - 데이터 파이프라인 기본형
- 데이터 웨어하우스를 중심으로 하는 데이터 파이프라인
- <img width="496" alt="image" src="https://github.com/led156/TIL/assets/67251510/4035945c-26a7-4265-8c9e-b04ec0e644ad">
- 데이터 웨어하우스는 (일반적인 RDB와는 달리) 대량의 데이터를 장기 보존하는 것에 최적화되어 있다.
- 정리된 데이터를 한 번에 전송하는 것은 뛰어나지만, 소량의 데이터를 자주 쓰고 읽는 데는 적합하지 않다.
- 데이터 웨어하우스의 측면에서 봤을 때
  + 데이터 소스(data source) : RDB나 로그 등을 저장하는 파일 서버
  + ETL 프로세스(ETL Process) : 데이터 소스에 보존된 로우 데이터를 추출하고 필요에 따라 가공한 후 데이터 웨어하우스에 저장하기까지의 흐름.
- 데이터 웨어하우스는 업무에 있어서 중요한 데이터 처리에 사용되기 때문에 아무때나 함부로 사용해 시스템에 과부하를 초래하는 것은 곤란함.
  + 데이터 마트(data mart) : 따라서, 데이터 분석과 같은 목적에 사용하는 경우에는 데이터웨어하우스에서 필요한 데이터만을 추출해 '데이터 마트'를 구축함.
- 데이터 웨어하우스, 데이터 마트 모두 SQL로 데이터를 집계.
  + 따라서, 먼저 테이블 설계를 제대로 정한 후에 데이터를 투입함.
  + 특히 BI 도구로 데이터를 볼 경우에는 미리 시각화에 적합한 형태로 테이블을 준비해야 함.
  + 그렇기 때문에 데이터 웨어하우스를 중심으로 하는 파이프라인에서는 테이블 설계와 ETL 프로세스가 중요함.
 
## 데이터 레이크 - 데이터를 그대로 축적
- 모든 데이터를 원래의 형태로 축적해두고 나중에 그것을 필요에 따라 가공하는 구조가 필요함. (모든 데이터가 데이터 웨어하우스를 가정해서 만들어지지 않기 때문에)
- <img width="489" alt="image" src="https://github.com/led156/TIL/assets/67251510/0e52eb5d-58d0-417a-b62b-ba4fb4491408">
- 데이터 레이크(data lake) : 데이터를 축적하는 호수. 데이터의 축적 장소.
  + 임의의 데이터를 저장할 수 있는 분산 스토리지가 데이터 레이크로 이용됨.
  + 데이터 형식은 자유지만, 대부분 CSV, JSON 등 범용적인 텍스트 형식이 사용된다.

### 데이터 레이크와 데이터 마트
- 데이터 레이크는 단순한 스토리지, 이것만으로는 데이터를 가공할 수 없음.
- 따라서 사용되는 것이 MapReduce 같은 분산 데이터 처리 기술임.
- 데이터 분석에 필요한 데이터를 가공, 집계하고, 이것을 데이터 마트로 추출한 후에는 데이터 웨어하우스 경우처럼 데이터 분석을 진행할 수 있음.

## 데이터 분석 기반을 단계적으로 발전시키기
<img width="437" alt="image" src="https://github.com/led156/TIL/assets/67251510/a8975c89-3cdb-437d-bc34-d3fe232efb68">

### 애드 혹 분석 및 대시보드 도구
- 애드 혹 분석 (일회성 데이터 분석) : 수작업에 해당됨.

### 데이터 마트와 워크플로 관리
- 복잡한 데이터 분석에서는 먼저 데이터 마트를 구축한 후에 분석하거나 시각화한다.
  + 시각화에 BI 도구를 사용할 경우는 집계 속도를 높이기 위해 데이터 마트가 거의 필수적.
  + 데이터 마트 구축은 배치 처리로 자동화되는 경우가 많으므로 실행 관리를 위해 워크플로 관리 도구를 사용한다.
 
### 데이터 파이프라인의 큰 흐름은 변하지 않는다.
- 데이터 파이프라인 전체의 기본 흐름은 변하지 않는다.
  + 데이터를 모아서 축적하고 이를 통합하여 데이터 마트로 만들고 시각화 도구에서 접속하는 것
  1. 저장할 수 있는 데이터 용량에 제한이 없을 것
  2. 데이터를 효율적으로 추출할 수단이 있을 것
  + 이 과정에서 기술은 교환할 수 있으므로, 전체 흐름을 총괄하는 워크플로 관리가 점차 중요해짐.

## 데이터를 수집하는 목적
<img width="477" alt="image" src="https://github.com/led156/TIL/assets/67251510/85fbb631-dcc1-4c60-a274-035f81bf3d01">

### 데이터 검색
- ❶ 데이터 검색 : 데이터 중에서 조건에 맞는 것을 찾고 싶은 경우
- 신속하게 검색할 수 있도록 해야하므로, 실시간 데이터 처리나 검색 엔진을 사용하여 키워드를 찾는 기능이 필요함.

### 데이터의 가공
- ❷ 데이터의 가공 : 데이터 처리 결과를 이용하고 싶은 경우
- 목적이 명확한 필요 데이터를 계획적으로 모아 데이터 파이프라인을 설계한다.
- 자동화가 필수적인 영역. 따라서 워크플로 관리를 도입하여 시스템을 구축함.

### 데이터 시각화
- ❸ 데이터 시각화 : 데이터를 시각적으로 봄으로써 정보를 얻는 것.

## 확증적 데이터 분석과 탐색적 데이터 분석
- 확증적 데이터 분석(confirmatory data analysis) : 가설을 세우고 그것을 검증함.
- 탐색적 데이터 분석(exploratory data analysis) : 데이터를 보면서 그 의미를 읽어내려고 함.

# 1.3. [속성 학습] 스크립트 언어에 의한 특별 분석과 데이터 프레임 (스몰데이터)
## 데이터 처리와 스크립트 언어

# 1.4. BI 도구와 모니터링

## 스프레드시트에 의한 모니터링
- 모니터링 : 정기적인 일정으로 동일한 집계를 반복하여 추이를 관찰.

## 데이터에 근거한 의사 결정
- KPI(key performance indicator) : 지표.

## 변화를 파악하고 세부 사항을 이해하기
- BI 도구는 자신이 직접 데이터를 보기 위한 소프트웨어이며, 집계의 단면을 다양하게 전환하면서 원하는 정보를 찾아낼 수 있다.

## 수작업과 자동화해야 할 것의 경계를 판별하기
### 자동화하려는 경우는 데이터 마트를 만든다
1. BI 도구에서 직접 데이터 소스에 접속하기
   - 장점 : 시스템 구성이 간단하다.
   - 단점 : BI 도구 측에서 지원하지 않는 데이터 소스에는 접속할 수 없다.
3. 데이터 마트를 준비하고, 그것을 BI 도구로부터 열기
   - 장점 : 어떤 테이블이라도 자유롭게 만들 수 있다.
   - 단점 : 데이터 마트의 설치 및 운영에 시간이 걸린다.
5. 웹 방식의 BI도구를 도입하여 CSV 파일을 업로드하기
   - 장점 : 스크립트로 자유롭게 데이터를 가공할 수 있다.
   - 단점 : 데이터의 생성 및 업로드에 프로그래밍이 필요하다.
